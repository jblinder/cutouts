{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "This is notebook is merged code from multiple notebooks. I'm in the process of cleaning the rest of the code but this is the general flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil.parser\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import matplotlib\n",
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "''' Settings '''\n",
    "sns.set_style(\"white\")\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.precision', 3)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load and concatinate CSV files into single dataframe'''\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Primary data\n",
    "csv_names = ['/home/ubuntu/dev/project04-foia/data/foia_root.csv']\n",
    "files = []\n",
    "for path in csv_names:\n",
    "    files.append(\n",
    "        pd.read_csv(path)\n",
    "    )\n",
    "df_root = pd.concat(files) \n",
    "\n",
    "# Comments\n",
    "csv_names = ['/home/ubuntu/dev/project04-foia/data/foia_comm.csv']\n",
    "files = []\n",
    "for path in csv_names:\n",
    "    files.append(\n",
    "        pd.read_csv(path)\n",
    "    )\n",
    "df_comm = pd.concat(files) \n",
    "\n",
    "# Files\n",
    "csv_names = ['/home/ubuntu/dev/project04-foia/data/foia_file.csv']\n",
    "files = []\n",
    "for path in csv_names:\n",
    "    files.append(\n",
    "        pd.read_csv(path)\n",
    "    )\n",
    "df_file = pd.concat(files) \n",
    "\n",
    "csv_names = ['/home/ubuntu/dev/project04-foia/data/foia_file_data_part.csv',\"/home/ubuntu/dev/project04-foia/data/foia_file_data.csv\",'/home/ubuntu/dev/project04-foia/data/foia_file_data_part2.csv']\n",
    "files = []\n",
    "for path in csv_names:\n",
    "    files.append(\n",
    "        pd.read_csv(path)\n",
    "    )\n",
    "df_file_data = pd.concat(files) \n",
    "\n",
    "df_root = df_root.reset_index()\n",
    "df_comm = df_comm.reset_index()\n",
    "df_file = df_file.reset_index()\n",
    "df_file_data = df_file_data.reset_index()\n",
    "\n",
    "df = pd.merge(df_root, df_comm, on='id_key', how='left', suffixes=('_root', '_comm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_names = ['/home/ubuntu/dev/project04-foia/data/foia_agency.csv']\n",
    "files = []\n",
    "for path in csv_names:\n",
    "    files.append(\n",
    "        pd.read_csv(path)\n",
    "    )\n",
    "df_gov = pd.concat(files)\n",
    "\n",
    "# Get agecny for each row\n",
    "def get_agency(row):\n",
    "    return str(df_gov[df_gov['id']==row]['name'])\n",
    "df_merge['agency_name'] = df_merge['agency'].apply(get_agency)\n",
    "df = df_merge\n",
    "\n",
    "# Dates\n",
    "df['datetime_done'] = pd.to_datetime(df['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topics for file data\n",
    "df = df.dropna(subset=['subject'])\n",
    "\n",
    "# Clean\n",
    "df['subject'] = df['subject'].str.replace('\\r', ' ')\n",
    "df['subject'] = df['subject'].str.replace('\\n', ' ')\n",
    "df['subject'] = df['subject'].replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "df['subject'] = df['subject'].replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "df['subject'] = df['subject'].str.lower()\n",
    "\n",
    "# Get topics for file data\n",
    "df = df.dropna(subset=['communication'])\n",
    "\n",
    "# Rough: remove names in validiction \n",
    "df['communication'] = df['communication'].str.rsplit(' ', 2).str[0]\n",
    "\n",
    "# Clean\n",
    "\n",
    "df['communication'] = df['communication'].str.replace('The requested documents will be made available to the general public and this request is not being made for commercial purposes', '')\n",
    "df['communication'] = df['communication'].str.replace('In the event that there are fees I would be grateful if you would inform me of the total charges in advance of fulfilling my request', '')\n",
    "df['communication'] = df['communication'].str.replace('I would prefer the request filled electronically by e-mail attachment if available or CD-ROM if not', '')\n",
    "df['communication'] = df['communication'].str.replace('Thank you in advance for your anticipated cooperation in this matter', '')\n",
    "df['communication'] = df['communication'].str.replace('I look forward to receiving your response to this request within 10 business days as the statute requires', '')\n",
    "\n",
    "\n",
    "df['communication'] = df['communication'].str.replace('To Whom It May Concern', '')\n",
    "df['communication'] = df['communication'].str.replace('Pursuant to the', '')\n",
    "df['communication'] = df['communication'].str.replace('I hereby request the following records', '')\n",
    "df['communication'] = df['communication'].str.replace('\\r', ' ')\n",
    "df['communication'] = df['communication'].str.replace('\\n', ' ')\n",
    "df['communication'] = df['communication'].replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "df['communication'] = df['communication'].replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "df['communication'] = df['communication'].str.lower()\n",
    "\n",
    "# Get topics for file data\n",
    "df = df_file_data\n",
    "df = df.dropna(subset=['file'])\n",
    "df['file'] = df['file'].str.replace('\\r', ' ')\n",
    "df['file'] = df['file'].str.replace('\\n', ' ')\n",
    "df['file'] = df['file'].replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "df['file'] = df['file'].replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "df['file'] = df['file'].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "df['subject_lemm'] = df['subject'].apply(lemmatize_text)\n",
    "df['communication_lemm'] = df['communication'].apply(lemmatize_text)\n",
    "df['file_lemm'] = df['file'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "words = ['To Whom It May Concern',\"nd\",\"st\",\"rd\", \"re\", 'Pursuant to the','Information Act', 'I hereby request the following records','The requested documents will be made available to the general public and this request is not being made for commercial purposes. In the event that there are fees I would be grateful if you would inform me of the total charges in advance of fulfilling my request. I would prefer the request filled electronically  by e-mail attachment if available or CD-ROM if not. Thank you in advance for your anticipated cooperation in this matter. I look forward to receiving your response to this request within 10 business days  as the statute requires','sincerely','working', 'request', 'dear', 'mr', 'request', 'hello', 'request', 'send', 'http', 'status', 'link', 'sorry', 'mrs', 'sir', 'view', 'request',\"Good morning\", \"Good Afternoon\",\"Hello\",\"hi\",\"Attached please find\",'The requested documents will be made available to the general public  and this request is not being made for commercial purposes','In the event that there are fees  I would be grateful if you would inform me of the total charges in advance of fulfilling my request. I would prefer the request filled electronically  by e-mail attachment if available or CD-ROM if not','Thank you in advance for your anticipated cooperation in this matter. I look forward to receiving your response to this request within 5 business days  as the statute requires']\n",
    "words.extend(\"hi respond completion estimated time search information time search information provide office know need response documents let number requested thank contact reference cost report attached like data foiaonline gov foia https action public message regulations application confirm information submission follows objectid home concern muckrock requests com foiaonline password account user days reset inactivity log sending application message happening prevent lock using page whomever obviously excellent efforts attention statistical local grounds contain good morning afternoon check follow instructions page right clicking evening foia records information office greetings agency gov number attachments receipt contact does subject date government email thanks thank sent address just check forward yes received week records new message added open closed www\".split(' '))\n",
    "words.extend(\"send problem got individual request mailto inquiry general help link dear mr ms request view working processing currently possible appreciate soon patience queue case actively assured committed additional details request following processed final affirmed acknowledge grant completely muckrock com requests\".split(' '))\n",
    "words.extend('clarification needed clarification needed wanted copied originally originally submitted submitted copied originally expect ntended recipient use intended recipient confidential error privileged prohibited requester cmailto cmailto cmailto  advance available fees total event filled prefer fulfilling advance fulfilling completed previously'.split(' '))\n",
    "words.extend('month later written format set works best considered appeal current  release likely expedited release terms people daily receive'.split(' '))\n",
    "words = [w.lower() for w in words]\n",
    "\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"unit\":\"suject_lemm\",\n",
    "          \"countv\":{\n",
    "            \"min_df\":.000025,\n",
    "            \"max_df\":0.1,\n",
    "            \"ngram_range\":(1,3)\n",
    "            },\n",
    "          \"tfidf\":{\n",
    "            \"min_df\":.000035,\n",
    "            \"max_df\":0.1,\n",
    "            \"ngram_range\":(1,3)\n",
    "            },\n",
    "          \"lda\":{\n",
    "              \"n_components\" : 25,\n",
    "              \"max_iter\": 30\n",
    "          },\n",
    "          \"nmf\":{\n",
    "              \"n_components\" : 25,\n",
    "              \"max_iter\": 200\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words=my_stop_words,\n",
    "    min_df=params['countv']['min_df'],\n",
    "    max_df=params['countv']['max_df'],\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "    ngram_range=params['countv']['ngram_range']\n",
    ")\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df[params['unit']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=my_stop_words,\n",
    "                                   min_df=params['tfidf']['min_df'],\n",
    "                                   max_df=params['tfidf']['max_df'],\n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   ngram_range=params['tfidf']['ngram_range'])\n",
    "tfidf_vectorizer_fit = tfidf_vectorizer.fit_transform(df[params['unit']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "lda = decomposition.LatentDirichletAllocation(\n",
    "    n_components=params['lda']['n_components'], \n",
    "    learning_method=\"batch\", \n",
    "    verbose=1, \n",
    "    max_iter=params['lda']['max_iter'],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lda.fit(count_vectorizer_fit)\n",
    "lda_model = lda.transform(count_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=params['nmf']['n_components'], \n",
    "          init='random', \n",
    "          max_iter=params['nmf']['max_iter'])\n",
    "nmf_fit = nmf.fit_transform(tfidf_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "def get_top_words_csv(model, feature_names, n_top_words,model_type,weight_type,col):\n",
    "    topics={}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        printmd(f\"**Topic #{topic_idx}**: {message}\")\n",
    "        topics[topic_idx] = message\n",
    "    df_temp = pd.DataFrame(list(topics.items()),columns=['topic_id', 'content'])\n",
    "    df_temp['type']         = col\n",
    "    df_temp['model']        = model_type\n",
    "    df_temp['n_components'] = params[model_type]['n_components']\n",
    "    df_temp['max_iter']     = params[model_type]['max_iter']\n",
    "    df_temp['weight']       = weight_type\n",
    "    df_temp['min_df']       = params[weight_type]['min_df']\n",
    "    df_temp['ngram_range']  = str(params[weight_type]['ngram_range'])\n",
    "    df_results.append(df_temp)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        printmd(f\"**Topic #{topic_idx}**: {message}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with pyLDAvis to help produce topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, count_vectorizer_fit, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current topics to csv\n",
    "\n",
    "get_top_words_csv(lda, list(count_vectorizer.get_feature_names()), 20,'lda','countv','subject')\n",
    "get_top_words_csv(nmf, list(tfidf_vectorizer.vocabulary_.keys()), 20,'nmf','tfidf','subject')\n",
    "\n",
    "# Generated topics\n",
    "topics = {\n",
    "0:\"Trump Imigration Policies\",\n",
    "1: \"Jenny Burke\",\n",
    "2: \"Palantir\",\n",
    "3: \"Massachusettes Police Protests\",\n",
    "4: \"Muslim Ban\",\n",
    "5: \"?\",\n",
    "6: \"Magnolia Patrol\",\n",
    "7: \"Police Drones\",\n",
    "8: \"Nesbitt Research\",\n",
    "9: \"Benedict Anderson\",    \n",
    "10: \"March On Washington\",\n",
    "11: \"Aircraft Attacks\",\n",
    "12: \"Defense Director\",\n",
    "13: \"?\",\n",
    "14: \"Homeland Security\",\n",
    "15: \"FBI Records\",\n",
    "16: \"Solitary Confinemnt\",\n",
    "17: \"Internet Surviellence\",\n",
    "18: \"Trump Transition\",\n",
    "19: \"FDA Food Ingredients\",\n",
    "20: \"Body Cam Footage\",\n",
    "21: \"Whistleblower\",\n",
    "22: \"Predictive Policing\",\n",
    "23: \"Transportation Security\",\n",
    "24: \"FCC & ICE\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topic weights\n",
    "def plot_topic_matrix(model,vectorizer_feature):\n",
    "\n",
    "   disp_topics = pd.DataFrame()\n",
    "   prob_comp = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "   for topic_idx, topic in enumerate(prob_comp):\n",
    "       top_indices = topic.argsort()[:-30 - 1:-1]\n",
    "       top_words = [f\"{vectorizer_feature[i]} {topic[i]:.1%}\" for i in top_indices]\n",
    "       disp_topics[f\"Topic {topic_idx}\"] = top_words\n",
    "   return disp_topics\n",
    "plot_topic_matrix(lda,count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topic weights\n",
    "top_n_words = 10\n",
    "t_words, word_strengths = {}, {}\n",
    "for t_id, t in enumerate(nmf.components_):\n",
    "    t_words[t_id] = [tfidf_vectorizer.get_feature_names()[i] for i in t.argsort()[:-top_n_words - 1:-1]]\n",
    "    word_strengths[t_id] = t[t.argsort()[:-top_n_words - 1:-1]]\n",
    "t_words\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,18), ncols=2, nrows=5)\n",
    "plt.subplots_adjust(\n",
    "    wspace  =  0.5,\n",
    "    hspace  =  0.5\n",
    ")\n",
    "c=0\n",
    "for row in range(0,5):\n",
    "    for col in range(0,2):\n",
    "        sns.barplot(x=word_strengths[c], y=t_words[c], color=\"red\", ax=ax[row][col])\n",
    "        c+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(lda_model[::40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tsne!\n",
    "doc_max_top = np.argmax(lda_model, axis=1 ) \n",
    "doc_max_top.shape\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(X_tsne[:,0],X_tsne[:,1], c=doc_max_top, alpha=1,s=2)\n",
    "plt.title(\"multicore-tsne + nmf + subject with max topic labels\");\n",
    "plt.legend(np.arange(1,10));\n",
    "#plt.savefig(\"tsne w euc.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "clustering = SpectralClustering(n_clusters=10)\n",
    "clusters = clustering.fit_predict(lda_model[::100])\n",
    "pca = PCA(2) \n",
    "projected = pca.fit_transform(lda_model[::100])\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=list(clusters), edgecolor='none', alpha=0.5)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();\n",
    "\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['x']=projected.T[0]\n",
    "data['y']=projected.T[1]\n",
    "data['labels']= list(clusters)\n",
    "\n",
    "facet = sns.lmplot(data=data, x='x', y='y', hue='labels', \n",
    "                   fit_reg=False, legend=True, legend_out=True, size=12, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "df = pd.read_csv('/home/ubuntu/dev/project04-foia/data/USE_THIS_DATAFRAME.csv')\n",
    "df = df.sort_values(by=['price'],ascending=False)\n",
    "df = df[df['price']!= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "df.sort_values(by=['price'],ascending=False)\n",
    "\n",
    "# group by price\n",
    "a = df.groupby(['topic'])['price'].median()\n",
    "a = pd.DataFrame(a).reset_index()\n",
    "a.columns = [\"topic\",\"price\"]\n",
    "a = a.sort_values(by=\"price\",ascending=True)\n",
    "a = a[10:]\n",
    "a = a[a[\"topic\"] != \"?\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,8),dpi=500,facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "plt.barh(a['topic'],a['price'],color='k')\n",
    "plt.xlabel(\"Media price per FOIA\")\n",
    "plt.ylabel(\"Topic\")\n",
    "\n",
    "csfont = {}\n",
    "hfont = {}\n",
    "\n",
    "plt.xticks(**hfont,fontsize=14)\n",
    "plt.yticks(**hfont,fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "for i, v in enumerate(a['price']):\n",
    "    ax.text(v+5 , i,' ${:,.2f}'.format(v),va=\"center\", color='blue', fontweight='bold')\n",
    "plt.xlim(10**0,10**6)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
